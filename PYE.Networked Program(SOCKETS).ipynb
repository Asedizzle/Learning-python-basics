{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP/1.1 200 OK\n",
      "Date: Wed, 01 Feb 2023 12:10:06 GMT\n",
      "Server: Apache/2.4.18 (Ubuntu)\n",
      "Last-Modified: Sat, 13 May 2017 11:22:22 GMT\n",
      "ETag: \"a7-54f6609245537\"\n",
      "Accept-Ranges: bytes\n",
      "Content-Length: 167\n",
      "Cache-Control: max-age=0, no-cache, no-store, must-revalidate\n",
      "Pragma: no-cache\n",
      "Expires: Wed, 11 Jan 1984 05:00:00 GMT\n",
      "Connection: close\n",
      "Content-Type: text/plain\n",
      "\n",
      "But soft what light through yonder window breaks\n",
      "It is the east and Juliet is the sun\n",
      "Arise fair sun and kill the envious moon\n",
      "Who is already sick and pale with grief\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import socket\n",
    "mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)#created socket linked socket to internet\n",
    "mysock.connect(('data.pr4e.org', 80)) #connect socket to port\n",
    "cmd = 'GET http://data.pr4e.org/romeo.txt HTTP/1.0\\r\\n\\r\\n'.encode()\n",
    "mysock.send(cmd) #This and line abovve for server computer\n",
    "\n",
    "while True:\n",
    "    data = mysock.recv(512)\n",
    "    if len(data) < 1:\n",
    "        break\n",
    "    print(data.decode(),end='') #From while true for client application\n",
    "\n",
    "mysock.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code was used to practice above info\n",
    "\"\"\"import socket\n",
    "mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)#created socket linked socket to internet\n",
    "mysock.connect(\"address\", 80) #show socket the path and which port\n",
    "\n",
    "cmd =\"is the data im going to get\".encode() \n",
    "mysock.send(cmd) \n",
    "\n",
    "while True:\n",
    "    data = mysock.recv(512) #how much should be recieved at a time\n",
    "    if len(data) < 1:\n",
    "        break\n",
    "    print(data.decode(), end= \"\")\n",
    "    \n",
    "mysock.close()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code was used to practice above info\n",
    "\"\"\"\n",
    "import socket\n",
    "mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "mysock.connect(\"web adress\", 80)\n",
    "\n",
    "cmd = \"GET whatever data\".encode()\n",
    "mysock.send(cmd)\n",
    "\n",
    "while True:\n",
    "    data = mysock.recv(512)\n",
    "    if len(data) < 1:\n",
    "        break\n",
    "    print(data.decode(), end= '')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But soft what light through yonder window breaks\n",
      "It is the east and Juliet is the sun\n",
      "Arise fair sun and kill the envious moon\n",
      "Who is already sick and pale with grief\n"
     ]
    }
   ],
   "source": [
    "#urllib is a library that does all the socket work for us,so it opens a url, reads through it and prints it\n",
    "\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "fhand = urllib.request.urlopen(\"\")\n",
    "for line in fhand:\n",
    "    print(line.decode().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But soft what light through yonder window breaks\n",
      "It is the east and Juliet is the sun\n",
      "Arise fair sun and kill the envious moon\n",
      "Who is already sick and pale with grief\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse,urllib.error\n",
    "fhand = urllib.request.urlopen(\"http://data.pr4e.org/romeo.txt\")\n",
    "for line in fhand:\n",
    "    print(line.decode().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B': 1, 'u': 6, 't': 12, ' ': 29, 's': 11, 'o': 8, 'f': 3, 'w': 4, 'h': 9, 'a': 10, 'l': 6, 'i': 13, 'g': 3, 'r': 7, 'y': 2, 'n': 9, 'd': 6, 'e': 12, 'b': 1, 'k': 3, 'I': 1, 'J': 1, 'A': 1, 'v': 1, 'm': 1, 'W': 1, 'c': 1, 'p': 1}\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "fhand = urllib.request.urlopen(\"http://data.pr4e.org/romeo.txt\")\n",
    "my_dict = {}\n",
    "for line in fhand:\n",
    "    words = (line.decode().strip())\n",
    "    for word in words:\n",
    "        my_dict[word] = my_dict.get(word, 0) +1\n",
    "print(my_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But soft what light through yonder window breaks\n",
      "It is the east and Juliet is the sun\n",
      "Arise fair sun and kill the envious moon\n",
      "Who is already sick and pale with grief\n",
      "{'B': 1, 'u': 6, 't': 12, ' ': 29, 's': 11, 'o': 8, 'f': 3, 'w': 4, 'h': 9, 'a': 10, 'l': 6, 'i': 13, 'g': 3, 'r': 7, 'y': 2, 'n': 9, 'd': 6, 'e': 12, 'b': 1, 'k': 3, 'I': 1, 'J': 1, 'A': 1, 'v': 1, 'm': 1, 'W': 1, 'c': 1, 'p': 1}\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "fhand = urllib.request.urlopen(\"http://data.pr4e.org/romeo.txt\")\n",
    "dict_2 = {}\n",
    "for line in fhand:\n",
    "    words = line.decode().strip()\n",
    "    print(words)\n",
    "    for word in words:\n",
    "        dict_2[word] = dict_2.get(word, 0) + 1\n",
    "print(dict_2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But soft what light through yonder window breaks\n",
      "It is the east and Juliet is the sun\n",
      "Arise fair sun and kill the envious moon\n",
      "Who is already sick and pale with grief\n",
      "{'B': 1, 'u': 6, 't': 12, ' ': 29, 's': 11, 'o': 8, 'f': 3, 'w': 4, 'h': 9, 'a': 10, 'l': 6, 'i': 13, 'g': 3, 'r': 7, 'y': 2, 'n': 9, 'd': 6, 'e': 12, 'b': 1, 'k': 3, 'I': 1, 'J': 1, 'A': 1, 'v': 1, 'm': 1, 'W': 1, 'c': 1, 'p': 1}\n"
     ]
    }
   ],
   "source": [
    "#This code reads a web page\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "fhand = urllib.request.urlopen(\"http://data.pr4e.org/romeo.txt\")\n",
    "cute = {}\n",
    "for line in fhand:\n",
    "    print(line.decode().strip())\n",
    "    words = line.decode().strip()\n",
    "    for word in words:\n",
    "        cute[word] = cute.get(word, 0) + 1\n",
    "print(cute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But soft what light through yonder window breaks\n",
      "It is the east and Juliet is the sun\n",
      "Arise fair sun and kill the envious moon\n",
      "Who is already sick and pale with grief\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "fhand = urllib.request.urlopen(\"http://data.pr4e.org/romeo.txt\")\n",
    "for line in fhand:\n",
    "    print(line.decode().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But soft what light through yonder window breaks\n",
      "It is the east and Juliet is the sun\n",
      "Arise fair sun and kill the envious moon\n",
      "Who is already sick and pale with grief\n",
      "{'B': 1, 'u': 6, 't': 12, ' ': 29, 's': 11, 'o': 8, 'f': 3, 'w': 4, 'h': 9, 'a': 10, 'l': 6, 'i': 13, 'g': 3, 'r': 7, 'y': 2, 'n': 9, 'd': 6, 'e': 12, 'b': 1, 'k': 3, 'I': 1, 'J': 1, 'A': 1, 'v': 1, 'm': 1, 'W': 1, 'c': 1, 'p': 1}\n"
     ]
    }
   ],
   "source": [
    "dict_44 = {}\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "fhand = urllib.request.urlopen(\"http://data.pr4e.org/romeo.txt\")\n",
    "for line in fhand:\n",
    "    words = line.decode().strip()\n",
    "    print(words)\n",
    "    for word in words:\n",
    "        dict_44[word] = dict_44.get(word,0)+1\n",
    "        \n",
    "print(dict_44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\program files\\python lives here\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\program files\\python lives here\\lib\\site-packages (from bs4) (4.11.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\program files\\python lives here\\lib\\site-packages (from beautifulsoup4->bs4) (2.3.2.post1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.dr-chuck.com/page2.htm\n"
     ]
    }
   ],
   "source": [
    "#code that reads through webpage and retrieves smth for you\n",
    "\n",
    "import urllib.request,urllib.parse,urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = input(\"Please enter valid url\")\n",
    "html = urllib.request.urlopen(url).read()\n",
    "\n",
    "soup = BeautifulSoup(html,\"html.parser\")\n",
    "\n",
    "#Retrieving all anchor tags\n",
    "tags = soup(\"a\")\n",
    "for tag in tags:\n",
    "    print(tag.get(\"href\", None))\n",
    "                     \n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://books.toscrape.com\n",
      "http://books.toscrape.com\n",
      "http://books.toscrape.com\n",
      "http://quotes.toscrape.com/\n",
      "http://quotes.toscrape.com\n",
      "http://quotes.toscrape.com/\n",
      "http://quotes.toscrape.com/scroll\n",
      "http://quotes.toscrape.com/js\n",
      "http://quotes.toscrape.com/js-delayed\n",
      "http://quotes.toscrape.com/tableful\n",
      "http://quotes.toscrape.com/login\n",
      "http://quotes.toscrape.com/search.aspx\n",
      "http://quotes.toscrape.com/random\n"
     ]
    }
   ],
   "source": [
    "urllib.request,urllib.parse,urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = input(\"Please enter a valid url\")\n",
    "html = urllib.request.urlopen(url).read()\n",
    "soup = BeautifulSoup(html,\"html.parser\")\n",
    "\n",
    "tags = soup(\"a\")\n",
    "for tag in tags:\n",
    "    print(tag.get(\"href\", None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request,urllib.parse,urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = input(\"Please enter a valid url\")\n",
    "html = urllib.request.urlopen(url).read()\n",
    "\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "tags = soup(\"a\")\n",
    "for tag in tags:\n",
    "    print(tag.get(\"href\", None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://books.toscrape.com\n",
      "http://books.toscrape.com\n",
      "http://books.toscrape.com\n",
      "http://quotes.toscrape.com/\n",
      "http://quotes.toscrape.com\n",
      "http://quotes.toscrape.com/\n",
      "http://quotes.toscrape.com/scroll\n",
      "http://quotes.toscrape.com/js\n",
      "http://quotes.toscrape.com/js-delayed\n",
      "http://quotes.toscrape.com/tableful\n",
      "http://quotes.toscrape.com/login\n",
      "http://quotes.toscrape.com/search.aspx\n",
      "http://quotes.toscrape.com/random\n"
     ]
    }
   ],
   "source": [
    "#Dr.Chuuck's example, full explanation at 7hr:15\n",
    "\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors,enables you to read through and parse https sites, https sites usually have cert that is not in python's offcial list so this bit of code=crucial\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = input('Enter - ')#ask user for url\n",
    "html = urllib.request.urlopen(url, context=ctx).read()#using urllib socket is created instead if looping through line by line .read(),goes through entire doc at once\n",
    "\n",
    "\"\"\"Beautiful soup go through entire doc and give us cleaned up version of the doc (since so many errors can be in a given html, \n",
    "along with the fact that there are different ways of writing for eg.anchor tags from the past, so go through the doc and return us a clean and fixen version\n",
    "of the html we're inputing. Fix all broken html.Thank God for the one who created beautifulsoup and went through all the things that could go wrong and fixed it\n",
    "\"\"\"\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Retrieve all of the anchor tags, returns a list of anchor tags with all the things inside em\n",
    "tags = soup('a')\n",
    "for tag in tags:\n",
    "    print(tag.get('href', None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://books.toscrape.com\n",
      "http://books.toscrape.com\n",
      "http://books.toscrape.com\n",
      "http://quotes.toscrape.com/\n",
      "http://quotes.toscrape.com\n",
      "http://quotes.toscrape.com/\n",
      "http://quotes.toscrape.com/scroll\n",
      "http://quotes.toscrape.com/js\n",
      "http://quotes.toscrape.com/js-delayed\n",
      "http://quotes.toscrape.com/tableful\n",
      "http://quotes.toscrape.com/login\n",
      "http://quotes.toscrape.com/search.aspx\n",
      "http://quotes.toscrape.com/random\n"
     ]
    }
   ],
   "source": [
    "#Code to check both http and hmtl sites.\n",
    "\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "abc = ssl.create_default_context()\n",
    "abc.check_hostname = False\n",
    "abc.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = input(\"Please enter a valid url\")\n",
    "html = urllib.request.urlopen(url , context=abc).read()\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "tags = soup(\"a\")\n",
    "for tag in tags:\n",
    "    print(tag.get(\"href\", None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://py4e-data.dr-chuck.net/known_by_Aniqa.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Ogheneruno.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Montgomery.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Owain.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Haniyah.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Anona.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Edyn.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Dallace.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Zoe.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Kiarash.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Tracy.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Carmyle.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Zahraa.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Alanys.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Airidas.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Melisa.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Vivian.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Margaret.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Hajra.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Ajooni.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Alexanne.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Sudais.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Seb.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Christin.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Jaimie.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Jennah.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Landon.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Mea.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Cacie.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Colton.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Mitchel.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Chintu.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Hyden.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Chrystal.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Lincon.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Jaden.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Roma.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Manolo.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Clio.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Teos.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Rihonn.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Griffin.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Conley.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Xiao.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Dhyia.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Manahil.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Diona.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Dharam.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Danielle.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Rori.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Lang.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Sabila.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Zoha.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Jemma.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Silvana.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Asal.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Dillon.html\n",
      "http://py4e-data.dr-chuck.net/known_by_CJ.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Joanna.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Atal.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Callun.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Anubhav.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Coray.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Graeme.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Chrissie.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Ayub.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Heather.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Katie.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Inaara.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Siddhant.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Salymat.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Shahd.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Anaya.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Kevaugh.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Thumbiko.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Xida.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Alaska.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Shonagh.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Kaiya.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Khadija.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Kieron.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Filip.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Dorothy.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Kallan.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Mena.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Abbie.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Amyleigh.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Annalise.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Carrich.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Rachel.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Etinosa.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Amie.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Lisa.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Liv.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Baylie.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Jubin.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Kacie.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Falyn.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Conli.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Cohen.html\n"
     ]
    }
   ],
   "source": [
    "import urllib.request,urllib.parse,urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "abc = ssl.create_default_context()\n",
    "abc.check_hostname = False\n",
    "abc.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = input(\"Please enter a valid url\")\n",
    "html = urllib.request.urlopen(url, context=abc).read()\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "tags = soup(\"a\")\n",
    "for tag in tags:\n",
    "    print(tag.get(\"href\", None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP/1.1 200 OK\n",
      "Date: Sat, 18 Feb 2023 22:36:16 GMT\n",
      "Server: Apache/2.4.18 (Ubuntu)\n",
      "Last-Modified: Sat, 13 May 2017 11:22:22 GMT\n",
      "ETag: \"1d3-54f6609240717\"\n",
      "Accept-Ranges: bytes\n",
      "Content-Length: 467\n",
      "Cache-Control: max-age=0, no-cache, no-store, must-revalidate\n",
      "Pragma: no-cache\n",
      "Expires: Wed, 11 Jan 1984 05:00:00 GMT\n",
      "Connection: close\n",
      "Content-Type: text/plain\n",
      "\n",
      "Why should you learn to write programs?\n",
      "\n",
      "Writing programs (or programming) is a very creative \n",
      "and rewarding activity.  You can write programs for \n",
      "many reasons, ranging from making your living to solving\n",
      "a difficult data analysis problem to having fun to helping\n",
      "someone else solve a problem.  This book assumes that \n",
      "everyone needs to know how to program, and that once \n",
      "you know how to program you will figure out what you want \n",
      "to do with your newfound skills.  \n"
     ]
    }
   ],
   "source": [
    "#Assignemnt from course\n",
    "import socket\n",
    "\n",
    "mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "mysock.connect(('data.pr4e.org', 80))\n",
    "cmd = 'GET http://data.pr4e.org/intro-short.txt HTTP/1.0\\r\\n\\r\\n'.encode()\n",
    "mysock.send(cmd)\n",
    "\n",
    "while True:\n",
    "    data = mysock.recv(512)\n",
    "    if len(data) < 1:\n",
    "        break\n",
    "    print(data.decode(),end='')\n",
    "\n",
    "mysock.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum is  2352\n"
     ]
    }
   ],
   "source": [
    "#Assignment from course, finds all span tags in html doc, converts their conyents(which are numbers) into integers and sums them up\n",
    "import urllib.request,urllib.parse,urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "this_1 = ssl.create_default_context()\n",
    "this_1.check_hostname = False\n",
    "this_1.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = \"http://py4e-data.dr-chuck.net/comments_1689412.html\"\n",
    "html = urllib.request.urlopen(url, context=this_1).read()\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "k = []\n",
    "tags = soup(\"span\")\n",
    "for tag in tags:\n",
    "    #print(tag.contents[0])\n",
    "    k.append(int(tag.contents[0]))\n",
    "    \n",
    "ans = sum(k)\n",
    "print(\"sum is \", ans)\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Tutortial\n",
    "# Retrieve all of the anchor tags\n",
    "tags = soup('a')\n",
    "for tag in tags:\n",
    "   # Look at the parts of a tag\n",
    "   print 'TAG:',tag\n",
    "   print 'URL:',tag.get('href', None)\n",
    "   print 'Contents:',tag.contents[0]\n",
    "   print 'Attrs:',tag.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://py4e-data.dr-chuck.net/known', 'by', 'Cohen.html']\n",
      "Cohen\n"
     ]
    }
   ],
   "source": [
    "import urllib.request,urllib.parse,urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "abc = ssl.create_default_context()\n",
    "abc.check_hostname = False\n",
    "abc.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = input(\"Please input a valid url\")\n",
    "html = urllib.request.urlopen(url, context=abc).read()\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "w = []\n",
    "tags = soup(\"a\")\n",
    "for tag in tags:\n",
    "    w.append(tag.get(\"href\", None))\n",
    "    \n",
    "#print(w)\n",
    "for i in w:\n",
    "    l = i.split(\"_\")\n",
    "    \n",
    "\n",
    "print(l)\n",
    "\n",
    "t = []\n",
    "t.append(l[2])\n",
    "\n",
    "for i in t:\n",
    "    g = i.split(\".\")\n",
    "    \n",
    "print(g[0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://py4e-data.dr-chuck.net/known_by_Montgomery.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Mhairade.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Butchi.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Anayah.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Anayah.html\n"
     ]
    }
   ],
   "source": [
    "#Practice problem\n",
    "import urllib.request,urllib.parse,urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "abc = ssl.create_default_context()\n",
    "abc.check_hostname = False\n",
    "abc.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "\n",
    "url = \"http://py4e-data.dr-chuck.net/known_by_Fikret.html\"\n",
    "html = urllib.request.urlopen(url, context=abc).read()\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "w = []\n",
    "tags = soup(\"a\")\n",
    "for tag in tags:\n",
    "    w.append(tag.get(\"href\", None))\n",
    "    \n",
    "#print(w)\n",
    "\n",
    "for i in range(4):\n",
    "    url = w[2]\n",
    "    #print(url)\n",
    "    html = urllib.request.urlopen(url, context=abc).read()\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    w = []\n",
    "    tags = soup(\"a\")\n",
    "    for tag in tags:\n",
    "        w.append(tag.get(\"href\", None))\n",
    "\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://py4e-data.dr-chuck.net/known_by_Moya.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Cosmo.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Oisin.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Lochlann.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Faria.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Raymond.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Elice.html\n"
     ]
    }
   ],
   "source": [
    "#Actual question....soln\n",
    "import urllib.request,urllib.parse,urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "abc = ssl.create_default_context()\n",
    "abc.check_hostname = False\n",
    "abc.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "\n",
    "url = input(\"Please enter a valid url\")\n",
    "html = urllib.request.urlopen(url, context=abc).read()\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "w = []\n",
    "tags = soup(\"a\")\n",
    "for tag in tags:\n",
    "    w.append(tag.get(\"href\", None))\n",
    "    \n",
    "#print(w)\n",
    "count = int(input(\"Please enter the number of times the process should be repeated\"))\n",
    "position = int(input(\"Please enter the position the next link can be found\"))\n",
    "final_position = position - 1\n",
    "for i in range(count):\n",
    "    url = w[final_position]\n",
    "    print(url)\n",
    "    html = urllib.request.urlopen(url, context=abc).read()\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    w = []\n",
    "    tags = soup(\"a\") \n",
    "    for tag in tags:\n",
    "        w.append(tag.get(\"href\", None))\n",
    "\n",
    "#print(\"Final Ans is\", url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2eda06f8fe6fb7abb511cc6e50ffbf584d0540cd226e02af9b351e301191bb81"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
